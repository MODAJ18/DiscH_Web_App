{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "B7cRAk-vtlrH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B7cRAk-vtlrH",
    "outputId": "bf1f3996-48bf-4527-80b5-130a960ddcc9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.8.0-py3-none-any.whl (452 kB)\n",
      "     -------------------------------------- 452.9/452.9 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "     ---------------------------------------- 5.8/5.8 MB 12.9 MB/s eta 0:00:00\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "     ---------------------------------------- 81.4/81.4 kB ? eta 0:00:00\n",
      "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
      "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "     ---------------------------------------- 182.4/182.4 kB ? eta 0:00:00\n",
      "Requirement already satisfied: aiohttp in c:\\users\\modaj\\anaconda3\\lib\\site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\modaj\\anaconda3\\lib\\site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\modaj\\anaconda3\\lib\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\modaj\\anaconda3\\lib\\site-packages (from datasets) (4.64.1)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "     -------------------------------------- 132.9/132.9 kB 8.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\modaj\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.1.0-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.5.2-cp39-cp39-win_amd64.whl (10.9 MB)\n",
      "     --------------------------------------- 10.9/10.9 MB 15.2 MB/s eta 0:00:00\n",
      "Collecting fsspec[http]>=2021.11.1\n",
      "  Downloading fsspec-2022.11.0-py3-none-any.whl (139 kB)\n",
      "     ---------------------------------------- 139.5/139.5 kB ? eta 0:00:00\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-10.0.1-cp39-cp39-win_amd64.whl (20.3 MB)\n",
      "     --------------------------------------- 20.3/20.3 MB 14.9 MB/s eta 0:00:00\n",
      "Collecting dill<0.3.7\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "     ---------------------------------------- 110.5/110.5 kB ? eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\modaj\\anaconda3\\lib\\site-packages (from datasets) (1.23.4)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2022.10.31-cp39-cp39-win_amd64.whl (267 kB)\n",
      "     ------------------------------------- 267.8/267.8 kB 17.2 MB/s eta 0:00:00\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 17.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\modaj\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\modaj\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\modaj\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\modaj\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\modaj\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\modaj\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\modaj\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\modaj\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\modaj\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\modaj\\anaconda3\\lib\\site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\modaj\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\modaj\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\modaj\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\modaj\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\modaj\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\modaj\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\modaj\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: tokenizers, xxhash, regex, pyarrow, fsspec, dill, responses, pandas, multiprocess, huggingface-hub, transformers, datasets, evaluate\n",
      "Successfully installed datasets-2.8.0 dill-0.3.6 evaluate-0.4.0 fsspec-2022.11.0 huggingface-hub-0.11.1 multiprocess-0.70.14 pandas-1.5.2 pyarrow-10.0.1 regex-2022.10.31 responses-0.18.0 tokenizers-0.13.2 transformers-4.25.1 xxhash-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef29ee8e",
   "metadata": {
    "id": "ef29ee8e"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cff9b0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfbda94",
   "metadata": {
    "id": "3bfbda94"
   },
   "source": [
    "## `data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebaf6f03",
   "metadata": {
    "id": "ebaf6f03"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import psycopg2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f12c8dcf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f12c8dcf",
    "outputId": "22eafb86-e144-410d-e839-d1991da72045"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dbname': 'd1mikus7g5uss8',\n",
       " 'host': 'ec2-99-81-68-240.eu-west-1.compute.amazonaws.com',\n",
       " 'port': '5432',\n",
       " 'user': 'iohznziolcottb',\n",
       " 'password': '5a812ea29f6142328bc2afab03e48e6462939babe87610342cdf12e2d357a4f0',\n",
       " 'sslmode': 'require'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "creds = {'dbname': 'd1mikus7g5uss8',\n",
    " 'host': 'ec2-99-81-68-240.eu-west-1.compute.amazonaws.com',\n",
    " 'port': '5432',\n",
    " 'user': 'iohznziolcottb',\n",
    " 'password': '5a812ea29f6142328bc2afab03e48e6462939babe87610342cdf12e2d357a4f0',\n",
    " 'sslmode': 'require'}\n",
    "\n",
    "creds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9534f527",
   "metadata": {
    "id": "9534f527"
   },
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host=creds['host'], database=creds['dbname'],\n",
    "                                user=creds['user'],\n",
    "                                password=creds['password'])\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c3bf1da",
   "metadata": {
    "id": "2c3bf1da"
   },
   "outputs": [],
   "source": [
    "select_Query = 'select * from public.\"DiscH_prototype_question\"'\n",
    "cur.execute(select_Query)\n",
    "questions = cur.fetchall()\n",
    "question_col = [\"question_id\", \"description\", \"num_response\", \"label_in_question\"]\n",
    "questions_df = pd.DataFrame(questions, columns=question_col)\n",
    "\n",
    "select_Query = 'select * from public.\"DiscH_prototype_answer\"'\n",
    "cur.execute(select_Query)\n",
    "answers = cur.fetchall()\n",
    "answer_col = [\"answer_id\", \"answer_category_num\", \"answer_justification\", \"answer_upvote\",\n",
    "              \"account_id_id\", \"question_id_id\", \"date\"]\n",
    "answers_df = pd.DataFrame(answers, columns=answer_col)\n",
    "\n",
    "select_Query = 'select * from public.\"DiscH_prototype_answer_bow\"'\n",
    "cur.execute(select_Query)\n",
    "answers_bow = cur.fetchall()\n",
    "answer_bow_col = [\"answer_id\", \"answer_category_num\", \"answer_justification\", \"answer_upvote\",\n",
    "              \"account_id_id\", \"question_id_id\", \"answer_text_comment\", \"date\"]\n",
    "answers_bow_df = pd.DataFrame(answers_bow, columns=answer_bow_col)\n",
    "\n",
    "\n",
    "select_Query = 'select * from public.\"DiscH_prototype_achievement\"'\n",
    "cur.execute(select_Query)\n",
    "achievements = cur.fetchall()\n",
    "achievement_col = [\"achievement_id\", \"achievement_type\", \"achievement_date\", \"account_id\", \"question_id_id\", \"value\"]\n",
    "achievements_df = pd.DataFrame(achievements, columns=achievement_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0289afd6",
   "metadata": {
    "id": "0289afd6"
   },
   "outputs": [],
   "source": [
    "select_Query = 'select q.description, a.answer_category_num, a.answer_justification, a.answer_upvote, \\\n",
    "              a.account_id_id from public.\"DiscH_prototype_question\" q RIGHT JOIN public.\"DiscH_prototype_answer\" a \\\n",
    "ON a.question_id_id=q.question_id'\n",
    "cur.execute(select_Query)\n",
    "QAs = cur.fetchall() \n",
    "QA_col = [\"question_description\", \"answer_category_num\", \"answer_justification\", \"answer_upvote\", \"account_id\"]\n",
    "QA_df = pd.DataFrame(QAs, columns=QA_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "O6PWt7wwxzA8",
   "metadata": {
    "id": "O6PWt7wwxzA8"
   },
   "outputs": [],
   "source": [
    "clean_data = QA_df.copy()\n",
    "clean_data  = clean_data[['question_description', 'answer_category_num']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ZJj7T3BzxvH_",
   "metadata": {
    "id": "ZJj7T3BzxvH_"
   },
   "outputs": [],
   "source": [
    "clean_data.drop_duplicates(subset='question_description', inplace=True)\n",
    "clean_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "UYh-YZ-ixhXR",
   "metadata": {
    "id": "UYh-YZ-ixhXR"
   },
   "outputs": [],
   "source": [
    "# checking how many comments have different votes\n",
    "for i, row in clean_data.iterrows():\n",
    "\n",
    "    comment = row.question_description\n",
    "\n",
    "    # getting the dataframe for that comment\n",
    "    temp_df = clean_data[clean_data.question_description == comment]\n",
    "    \n",
    "    # how many unique answers that comment has\n",
    "    n_ = temp_df.answer_category_num.nunique()\n",
    "\n",
    "    # changing the answer_category_num to the most frequent one\n",
    "    if n_ >= 2:\n",
    "        most_voted_label = temp_df.answer_category_num.value_counts().index[0]\n",
    "        clean_data.at[i, 'answer_category_num'] = most_voted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "rJxbnEWZ03wM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "rJxbnEWZ03wM",
    "outputId": "3f8d13d8-fe35-4ffd-e345-b85ea7476151"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_description</th>\n",
       "      <th>answer_category_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>وزير الخارجية اللبناني جبران باسيل قال في سلسل...</td>\n",
       "      <td>Religious affiliation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>سورية بلد الحضارات تربطها بعلية او بحيوان</td>\n",
       "      <td>Violent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>تقتلون وسام الحسن وتترحموعلية من أي أصناف المخ...</td>\n",
       "      <td>Racist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>معك خبر انو بلدة قطر متل ما سميتا مساحتها اكبر...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>للامانه قوت الموسم اللي طاف كان هوا بس متحمس ح...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>اجواء اوروبيه بواقع صومالي زنجي حقير</td>\n",
       "      <td>Racist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>قداي بدك نمرة الضبان خبرني حتى شوف اذا في قياس...</td>\n",
       "      <td>Violent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>بغض النظر عن صحّة ترجمة تصريح جبران باسيل لل ،...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>هودي ما رح يتغيّروا وما تغيّروا</td>\n",
       "      <td>Mockery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2264</th>\n",
       "      <td>وانا اتمنى يا وهاب الكلب عدم توجيه كلامك الى م...</td>\n",
       "      <td>Violent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2265 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   question_description    answer_category_num\n",
       "0     وزير الخارجية اللبناني جبران باسيل قال في سلسل...  Religious affiliation\n",
       "1             سورية بلد الحضارات تربطها بعلية او بحيوان                Violent\n",
       "2     تقتلون وسام الحسن وتترحموعلية من أي أصناف المخ...                 Racist\n",
       "3     معك خبر انو بلدة قطر متل ما سميتا مساحتها اكبر...                 Normal\n",
       "4     للامانه قوت الموسم اللي طاف كان هوا بس متحمس ح...                 Normal\n",
       "...                                                 ...                    ...\n",
       "2260               اجواء اوروبيه بواقع صومالي زنجي حقير                 Racist\n",
       "2261  قداي بدك نمرة الضبان خبرني حتى شوف اذا في قياس...                Violent\n",
       "2262  بغض النظر عن صحّة ترجمة تصريح جبران باسيل لل ،...                 Normal\n",
       "2263                    هودي ما رح يتغيّروا وما تغيّروا                Mockery\n",
       "2264  وانا اتمنى يا وهاب الكلب عدم توجيه كلامك الى م...                Violent\n",
       "\n",
       "[2265 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b146883",
   "metadata": {},
   "source": [
    "## `Model #1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7jvq_QmayQh8",
   "metadata": {
    "id": "7jvq_QmayQh8"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m      3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(clean_data)\u001b[38;5;241m.\u001b[39mtrain_test_split(test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(clean_data).train_test_split(test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "zL5UxD8E02Lo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zL5UxD8E02Lo",
    "outputId": "c59eaa15-bd82-47b2-9941-84966dc2d5fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question_description', 'answer_category_num'],\n",
       "        num_rows: 1916\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question_description', 'answer_category_num'],\n",
       "        num_rows: 339\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "G4Rtr_TF2K5F",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "36d79cb206b24af8b45970f2da5d52b2",
      "d86118687d64415091e331cf38bc1b3b",
      "2f8e532fe4734999906b3a300cc42f5b",
      "cf9d1caaf9074e20af4312cf2729226b",
      "b229ea6d57114544b817da8d41e8185a",
      "d2390de8f99a4f33941d99ee10178f21",
      "8d187893389d4c9fb2ea5c36de0d07b0",
      "f02ed400ea7a4f32b9342d90413a3bd4",
      "df5a7e8cd4fb4fa68d915d5d831e8f35",
      "817734344829447ea2b02a2b450966fd",
      "81d0e0dd48dc4d4f852e9405dd89a2cd",
      "6da82680dd0d40888347b72314ce3872",
      "1f64c49e5d2c40c99a1092406df06e8d",
      "e39e46a9cfe6461da49e3f4b759504bf",
      "a241863ddbbb4c5e8d79a5ee7c677085",
      "d41734acd9274ed69199a882ce31b225",
      "300a9a70c8a54f03bc1a2da69222c50c",
      "3f76c5c6bf604a5ebfbdbb87a7550ce9",
      "6acacf94af3140aaafb88c54de85f141",
      "283365b5551b4282adbbb2aa8ba78408",
      "389d381a2457446684dc844e0bc84454",
      "fb5c6e9c169745a1a7bdab62c7774a73"
     ]
    },
    "id": "G4Rtr_TF2K5F",
    "outputId": "5e83cf9f-7dd3-400e-b3b6-37b608de19f6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d79cb206b24af8b45970f2da5d52b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da82680dd0d40888347b72314ce3872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"question_description\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "33qLcWnZ4BTs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33qLcWnZ4BTs",
    "outputId": "87692247-069e-4eda-a86c-4cd2a2975565"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2BTX_-z-4c-0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2BTX_-z-4c-0",
    "outputId": "1232ec00-47aa-458b-b498-7d5edca0500e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", num_train_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "-jsDiUJL5eSZ",
   "metadata": {
    "id": "-jsDiUJL5eSZ"
   },
   "outputs": [],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"] \n",
    "eval_dataset = tokenized_datasets[\"test\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "CwYe1nkK4iVC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "CwYe1nkK4iVC",
    "outputId": "8935b81b-823a-4825-e95f-0ea4706cec6d",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-e5892c20831c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m trainer = Trainer(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplace_model_on_device\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m         \u001b[0;31m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mParallelMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tie_weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1680\u001b[0m             )\n\u001b[1;32m   1681\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    983\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    984\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 14.76 GiB total capacity; 13.86 GiB already allocated; 73.75 MiB free; 13.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16775948",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers evaluate\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "## `data`\n",
    "\n",
    "import subprocess\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "creds = {'dbname': 'd1mikus7g5uss8',\n",
    " 'host': 'ec2-99-81-68-240.eu-west-1.compute.amazonaws.com',\n",
    " 'port': '5432',\n",
    " 'user': 'iohznziolcottb',\n",
    " 'password': '5a812ea29f6142328bc2afab03e48e6462939babe87610342cdf12e2d357a4f0',\n",
    " 'sslmode': 'require'}\n",
    "\n",
    "creds\n",
    "\n",
    "conn = psycopg2.connect(host=creds['host'], database=creds['dbname'],\n",
    "                                user=creds['user'],\n",
    "                                password=creds['password'])\n",
    "cur = conn.cursor()\n",
    "\n",
    "select_Query = 'select * from public.\"DiscH_prototype_question\"'\n",
    "cur.execute(select_Query)\n",
    "questions = cur.fetchall()\n",
    "question_col = [\"question_id\", \"description\", \"num_response\", \"label_in_question\"]\n",
    "questions_df = pd.DataFrame(questions, columns=question_col)\n",
    "\n",
    "select_Query = 'select * from public.\"DiscH_prototype_answer\"'\n",
    "cur.execute(select_Query)\n",
    "answers = cur.fetchall()\n",
    "answer_col = [\"answer_id\", \"answer_category_num\", \"answer_justification\", \"answer_upvote\",\n",
    "              \"account_id_id\", \"question_id_id\", \"date\"]\n",
    "answers_df = pd.DataFrame(answers, columns=answer_col)\n",
    "\n",
    "select_Query = 'select * from public.\"DiscH_prototype_answer_bow\"'\n",
    "cur.execute(select_Query)\n",
    "answers_bow = cur.fetchall()\n",
    "answer_bow_col = [\"answer_id\", \"answer_category_num\", \"answer_justification\", \"answer_upvote\",\n",
    "              \"account_id_id\", \"question_id_id\", \"answer_text_comment\", \"date\"]\n",
    "answers_bow_df = pd.DataFrame(answers_bow, columns=answer_bow_col)\n",
    "\n",
    "\n",
    "select_Query = 'select * from public.\"DiscH_prototype_achievement\"'\n",
    "cur.execute(select_Query)\n",
    "achievements = cur.fetchall()\n",
    "achievement_col = [\"achievement_id\", \"achievement_type\", \"achievement_date\", \"account_id\", \"question_id_id\", \"value\"]\n",
    "achievements_df = pd.DataFrame(achievements, columns=achievement_col)\n",
    "\n",
    "select_Query = 'select q.description, a.answer_category_num, a.answer_justification, a.answer_upvote, \\\n",
    "              a.account_id_id from public.\"DiscH_prototype_question\" q RIGHT JOIN public.\"DiscH_prototype_answer\" a \\\n",
    "ON a.question_id_id=q.question_id'\n",
    "cur.execute(select_Query)\n",
    "QAs = cur.fetchall() \n",
    "QA_col = [\"question_description\", \"answer_category_num\", \"answer_justification\", \"answer_upvote\", \"account_id\"]\n",
    "QA_df = pd.DataFrame(QAs, columns=QA_col)\n",
    "\n",
    "clean_data = QA_df.copy()\n",
    "clean_data  = clean_data[['question_description', 'answer_category_num']]\n",
    "\n",
    "clean_data.drop_duplicates(subset='question_description', inplace=True)\n",
    "clean_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# checking how many comments have different votes\n",
    "for i, row in clean_data.iterrows():\n",
    "\n",
    "    comment = row.question_description\n",
    "\n",
    "    # getting the dataframe for that comment\n",
    "    temp_df = clean_data[clean_data.question_description == comment]\n",
    "    \n",
    "    # how many unique answers that comment has\n",
    "    n_ = temp_df.answer_category_num.nunique()\n",
    "\n",
    "    # changing the answer_category_num to the most frequent one\n",
    "    if n_ >= 2:\n",
    "        most_voted_label = temp_df.answer_category_num.value_counts().index[0]\n",
    "        clean_data.at[i, 'answer_category_num'] = most_voted_label\n",
    "\n",
    "clean_data\n",
    "\n",
    "## `2) huggingface`\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(clean_data).train_test_split(test_size=0.15)\n",
    "\n",
    "dataset\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"question_description\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=6)\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", num_train_epochs=10)\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"] \n",
    "eval_dataset = tokenized_datasets[\"test\"] \n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataEngineeringKER",
   "language": "python",
   "name": "dataengineeringker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
